{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff386d2",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ff76e",
   "metadata": {},
   "source": [
    "# 9.0 Enabling GPU within a Kubernetes (K8s) Cluster\n",
    "## (part of Lab 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac66b5",
   "metadata": {},
   "source": [
    "<img src=\"images/k8s/kubernetes_stack_0.png\" style=\"float: right;\">\n",
    "In this notebook, you'll learn how to prepare a Kubernetes cluster for GPU acceleration full production deployment of conversational AI applications.<br><br>\n",
    "\n",
    "**[9.1 Launch a K8s Cluster](#9.1-Launch-a-K8s-Cluster)<br>**\n",
    "**[9.2 Deploy a CUDA Test Application](#9.2-Deploy-a-CUDA-Test-Application)<br>**\n",
    "**[9.3 Add GPU Awareness to K8s](#9.3-Add-GPU-Awareness-to-K8s)<br>**\n",
    "**[9.4 Interact with GPU Resources in K8s](#9.4-Interact-with-GPU-Resources-in-K8s)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[9.4.1 Exercise: Configure Pod](#9.4.1-Exercise:-Configure-Pod)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[9.4.2 Final Checks and Shutdown](#9.4.2-Final-Checks-and-Shutdown)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[9.4.2.1 Exercise: Delete a Pod](#9.4.2.1-Exercise:-Delete-a-Pod)<br>\n",
    "\n",
    "In the previous parts of the class, you deployed NVIDIA Riva using basic shell commands. As convenient as this method is during development, it becomes impractical when deploying to production, that is, when managing larger numbers of servers and services. \n",
    "\n",
    "[Kubernetes](https://kubernetes.io/), also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications. \n",
    "In this part of the class, we will first launch a K8s cluster, enable the cluster for GPU acceleration and interact with those resources. This is our first step toward monitoring, managing, and deploying conversational AI applications in production. Monitoring and deployment will be covered in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e938dc",
   "metadata": {},
   "source": [
    "### Notebook Dependencies\n",
    "The steps in this notebook assume that you are starting with a clean environment.  Ensure that by stopping any previous Kubernetes installation and all docker containers, then looking at our environment's state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf7061d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Check running docker containers. This should be empty.\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f8af4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"docker kill\" requires at least 1 argument.\n",
      "See 'docker kill --help'.\n",
      "\n",
      "Usage:  docker kill [OPTIONS] CONTAINER [CONTAINER...]\n",
      "\n",
      "Kill one or more running containers\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# If not empty,\n",
    "# Clear Docker containers to start fresh...\n",
    "!docker kill $(docker ps -q)\n",
    "\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997fc5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🙄  \"minikube\" profile does not exist, trying anyways.\n",
      "💀  Removed all traces of the \"minikube\" cluster.\n"
     ]
    }
   ],
   "source": [
    "# Deletes local Kubernetes cluster if it exists\n",
    "!minikube delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3e00d",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.1 Launch a K8s Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688d884",
   "metadata": {},
   "source": [
    "A [Kubernetes cluster](https://kubernetes.io/docs/concepts/overview/components/) consists of a set of worker machines (physical or virtual), called nodes, that run containerized applications. Every cluster has at least one worker node, though it can also support thousands of nodes! For this class, we will use [Minikube](https://minikube.sigs.k8s.io/docs/), which allows us to deploy a local and self-contained Kubernetes cluster with a single node. \n",
    "\n",
    "Review the class hardware resources available and launch the K8s cluster.\n",
    "\n",
    "We can see details and status of the available GPU using the `nvidia-smi` command.\n",
    "\n",
    "<img src=\"images/k8s/nvidia_smi.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39794855",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 27 07:38:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   34C    P8    17W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# What GPU are we using and how much memory does it have?\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbea1871",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name\t: AMD EPYC 7R32\n",
      "model name\t: AMD EPYC 7R32\n",
      "model name\t: AMD EPYC 7R32\n",
      "model name\t: AMD EPYC 7R32\n"
     ]
    }
   ],
   "source": [
    "# What type of CPU processor(s) are we using?\n",
    "!cat /proc/cpuinfo | grep \"model name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ea6814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# How many processors are available?\n",
    "!nproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a7ba33",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄  minikube v1.23.2 on Ubuntu 20.04 (docker/amd64)\n",
      "✨  Using the none driver based on user configuration\n",
      "👍  Starting control plane node minikube in cluster minikube\n",
      "🤹  Running on localhost (CPUs=4, Memory=15818MB, Disk=297738MB) ...\n",
      "ℹ️  OS release is Ubuntu 20.04.5 LTS\n",
      "    > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n",
      "    > kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n",
      "    > kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s\n",
      "    > kubeadm: 43.71 MiB / 43.71 MiB [------------] 100.00% 95.66 MiB p/s 700msK\u001b[K/ K\u001b[K\u001b[K\u001b[K- \n",
      "    > kubectl: 44.73 MiB / 44.73 MiB [-----------] 100.00% 106.56 MiB p/s 600msK\u001b[K\\ \n",
      "    > kubelet: 146.25 MiB / 146.25 MiB [-----------] 100.00% 98.14 MiB p/s 1.7sK\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K/ \n",
      "\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "    ▪ Generating certificates and keys ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "    ▪ Booting up control plane ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "    ▪ Configuring RBAC rules ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "🤹  Configuring local host environment ...\n",
      "\n",
      "❗  The 'none' driver is designed for experts who need to integrate with an existing VM\n",
      "💡  Most users should use the newer 'docker' driver instead, which does not require root!\n",
      "📘  For more information, see: https://minikube.sigs.k8s.io/docs/reference/drivers/none/\n",
      "\n",
      "❗  kubectl and minikube configuration will be stored in /root\n",
      "❗  To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run:\n",
      "\n",
      "    ▪ sudo mv /root/.kube /root/.minikube $HOME\n",
      "    ▪ sudo chown -R $USER $HOME/.kube $HOME/.minikube\n",
      "\n",
      "💡  This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true\n",
      "🔎  Verifying Kubernetes components...\n",
      "    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "🌟  Enabled addons: default-storageclass, storage-provisioner\n",
      "🏄  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n"
     ]
    }
   ],
   "source": [
    "# Launch the K8s cluster using Minikube\n",
    "!minikube start --driver=none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97deda3",
   "metadata": {},
   "source": [
    "Once the cluster is successfully launched, we expect to see a number of containers running.  Check this by executing `docker ps` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62eeaf4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                  COMMAND                  CREATED          STATUS         PORTS     NAMES\n",
      "6fc49fbf4a4c   e64579b7d886           \"kube-apiserver --ad…\"   9 seconds ago    Up 8 seconds             k8s_kube-apiserver_kube-apiserver-261913e3dab5_kube-system_165a0116aebb93e175f751846787d23c_0\n",
      "7599860148c6   004811815584           \"etcd --advertise-cl…\"   9 seconds ago    Up 8 seconds             k8s_etcd_etcd-261913e3dab5_kube-system_28d51804f19478b4a9d2ff7e7e782917_0\n",
      "9d38d77d1066   5425bcbd23c5           \"kube-controller-man…\"   9 seconds ago    Up 8 seconds             k8s_kube-controller-manager_kube-controller-manager-261913e3dab5_kube-system_d542bcea397b77607da4f7ef2f47861c_0\n",
      "57008fb8c12d   b51ddc1014b0           \"kube-scheduler --au…\"   9 seconds ago    Up 8 seconds             k8s_kube-scheduler_kube-scheduler-261913e3dab5_kube-system_fdc4bb59feb740feccae609b8e626d94_0\n",
      "da8c97f1e66a   k8s.gcr.io/pause:3.5   \"/pause\"                 19 seconds ago   Up 8 seconds             k8s_POD_kube-apiserver-261913e3dab5_kube-system_165a0116aebb93e175f751846787d23c_0\n",
      "aa17a96e981a   k8s.gcr.io/pause:3.5   \"/pause\"                 19 seconds ago   Up 8 seconds             k8s_POD_etcd-261913e3dab5_kube-system_28d51804f19478b4a9d2ff7e7e782917_0\n",
      "259b8ccdb45b   k8s.gcr.io/pause:3.5   \"/pause\"                 19 seconds ago   Up 8 seconds             k8s_POD_kube-scheduler-261913e3dab5_kube-system_fdc4bb59feb740feccae609b8e626d94_0\n",
      "de4bb0c5e03b   k8s.gcr.io/pause:3.5   \"/pause\"                 19 seconds ago   Up 8 seconds             k8s_POD_kube-controller-manager-261913e3dab5_kube-system_d542bcea397b77607da4f7ef2f47861c_0\n"
     ]
    }
   ],
   "source": [
    "# Listing the Kuberenetes components deployed\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb0e1b",
   "metadata": {},
   "source": [
    "We should now have access to the [kubectl command line tool](https://kubernetes.io/docs/reference/kubectl/overview/), which is used to interact with the cluster. List the nodes and services in the cluster using the `kubectl get` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a695a91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           STATUS     ROLES                  AGE   VERSION\n",
      "261913e3dab5   NotReady   control-plane,master   5s    v1.22.2\n"
     ]
    }
   ],
   "source": [
    "# List nodes in the cluster\n",
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa19ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4s\n"
     ]
    }
   ],
   "source": [
    "# List all services deployed\n",
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93400d38",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.2 Deploy a CUDA Test Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e6d51",
   "metadata": {},
   "source": [
    "Next, we will deploy a simple GPU-accelerated application. This is a toy application which randomly generates two very large vectors and adds them. Print out the YAML configuration file needed to deploy the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beecc3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the configuration directory\n",
    "CONFIG_DIR='/dli/task/kubernetes-config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b6e41d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: gpu-operator-test\n",
      "spec:\n",
      "  restartPolicy: OnFailure\n",
      "  containers:\n",
      "  - name: cuda-vector-add\n",
      "    image: \"nvidia/samples:vectoradd-cuda11.6.0\"\n",
      "    resources:\n",
      "      limits:\n",
      "         nvidia.com/gpu: 1"
     ]
    }
   ],
   "source": [
    "# Review the application we will deploy\n",
    "!cat $CONFIG_DIR/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac0bf4",
   "metadata": {},
   "source": [
    "The main difference between a YAML file specifying a GPU-accelerated application compared to one specifying a non-GPU-accelerated application, is the configuration of the GPU resources required. In our case, we have created a basic configuration requesting a single NVIDIA GPU by setting `resources: limits: nvidia.com/gpu:` to 1. \n",
    "\n",
    "To deploy an application, execute the `kubectl apply` command, specifying the YAML configuration file with the `-f` file option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95967dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (Forbidden): error when creating \"/dli/task/kubernetes-config/gpu-pod.yaml\": pods \"gpu-operator-test\" is forbidden: error looking up service account default/default: serviceaccount \"default\" not found\n"
     ]
    }
   ],
   "source": [
    "# Deploy the application\n",
    "!kubectl apply -f $CONFIG_DIR/gpu-pod.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f808f91",
   "metadata": {},
   "source": [
    "Once deployed, we can observe the status of a pod created with `kubectl get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde467c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test\" not found\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76183d79",
   "metadata": {},
   "source": [
    "At this stage, the application is in the \"Pending\" state. <br>\n",
    "Why do you think this is case? Do you think its just the fact we have not given the application enough time to launch? Or do you think there are other reasons for this behavior? Try executing the same command again to see if the status changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c34d59c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test\" not found\n"
     ]
    }
   ],
   "source": [
    "# Checking again. Is it still pending?\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db584784",
   "metadata": {},
   "source": [
    "So the application is indeed in the \"Pending\" state and it will remain like that irrespective of the amount of time we wait. Why? Begin to answer this by looking at the configuration of the available nodes (in our case we just have one). In particular, look for any NVIDIA-specific configuration using the `kubectl describe` command, as this will help us identify GPU resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02ca86b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               261913e3dab5\n",
      "Roles:              control-plane,master\n",
      "Labels:             beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=amd64\n",
      "                    kubernetes.io/hostname=261913e3dab5\n",
      "                    kubernetes.io/os=linux\n",
      "                    minikube.k8s.io/commit=0a0ad764652082477c00d51d2475284b5d39ceed\n",
      "                    minikube.k8s.io/name=minikube\n",
      "                    minikube.k8s.io/updated_at=2023_07_27T07_39_36_0700\n",
      "                    minikube.k8s.io/version=v1.23.2\n",
      "                    node-role.kubernetes.io/control-plane=\n",
      "                    node-role.kubernetes.io/master=\n",
      "                    node.kubernetes.io/exclude-from-external-load-balancers=\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Thu, 27 Jul 2023 07:39:33 +0000\n",
      "Taints:             node.kubernetes.io/not-ready:NoSchedule\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  261913e3dab5\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Thu, 27 Jul 2023 07:39:36 +0000\n",
      "Conditions:\n",
      "  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----             ------  -----------------                 ------------------                ------                       -------\n",
      "  MemoryPressure   False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure     False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure      False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready            False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:36 +0000   KubeletNotReady              container runtime status check may not have completed yet\n",
      "Addresses:\n",
      "  InternalIP:  172.18.0.2\n",
      "  Hostname:    261913e3dab5\n",
      "Capacity:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16197972Ki\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16197972Ki\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 996614ec4c814b87b7ec8ebee3d0e8c9\n",
      "  System UUID:                352b2c31-b1a8-4d22-a7dc-e74746eec601\n",
      "  Boot ID:                    e28f248e-77f5-449d-a363-e61b694bca4e\n",
      "  Kernel Version:             5.11.0-1028-aws\n",
      "  OS Image:                   Ubuntu 20.04.5 LTS\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  docker://23.0.2\n",
      "  Kubelet Version:            v1.22.2\n",
      "  Kube-Proxy Version:         v1.22.2\n",
      "Non-terminated Pods:          (4 in total)\n",
      "  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n",
      "  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                 etcd-261913e3dab5                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         4s\n",
      "  kube-system                 kube-apiserver-261913e3dab5             250m (6%)     0 (0%)      0 (0%)           0 (0%)         4s\n",
      "  kube-system                 kube-controller-manager-261913e3dab5    200m (5%)     0 (0%)      0 (0%)           0 (0%)         4s\n",
      "  kube-system                 kube-scheduler-261913e3dab5             100m (2%)     0 (0%)      0 (0%)           0 (0%)         5s\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests    Limits\n",
      "  --------           --------    ------\n",
      "  cpu                650m (16%)  0 (0%)\n",
      "  memory             100Mi (0%)  0 (0%)\n",
      "  ephemeral-storage  0 (0%)      0 (0%)\n",
      "  hugepages-1Gi      0 (0%)      0 (0%)\n",
      "  hugepages-2Mi      0 (0%)      0 (0%)\n",
      "Events:\n",
      "  Type    Reason                   Age                From     Message\n",
      "  ----    ------                   ----               ----     -------\n",
      "  Normal  NodeHasSufficientMemory  21s (x4 over 21s)  kubelet  Node 261913e3dab5 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    21s (x4 over 21s)  kubelet  Node 261913e3dab5 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     21s (x3 over 21s)  kubelet  Node 261913e3dab5 status is now: NodeHasSufficientPID\n",
      "  Normal  Starting                 4s                 kubelet  Starting kubelet.\n",
      "  Normal  NodeHasSufficientMemory  4s                 kubelet  Node 261913e3dab5 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    4s                 kubelet  Node 261913e3dab5 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     4s                 kubelet  Node 261913e3dab5 status is now: NodeHasSufficientPID\n",
      "  Normal  NodeNotReady             4s                 kubelet  Node 261913e3dab5 status is now: NodeNotReady\n",
      "  Normal  NodeAllocatableEnforced  4s                 kubelet  Updated Node Allocatable limit across pods\n"
     ]
    }
   ],
   "source": [
    "# Can we see the GPU?\n",
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b3b5ed",
   "metadata": {},
   "source": [
    "Can you find anything? Try again, filtering the output with `grep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ce95571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look for the lines containing the word \"nvidia\"\n",
    "!kubectl describe nodes | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f17dd6",
   "metadata": {},
   "source": [
    "We did not find anything. That would explain why the application is still pending. Our cluster is not aware of the presence of the GPU.  The cluster is unable to schedule the execution since our YAML required GPU resources, but they are for all intents and purposes unavailable. We need to add the NVIDIA GPU device plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a09da",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.3 Add GPU Awareness to K8s\n",
    "To take advantage of GPU acceleration on Kubernetes, install the [NVIDIA GPU plugin](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#deploying-nvidia-gpu-device-plugin) to the cluster. Before adding it, look at the status without the plugin  with `kubectl get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e5d1b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "kube-system   etcd-261913e3dab5                      0/1     Running   0          5s\n",
      "kube-system   kube-apiserver-261913e3dab5            0/1     Running   0          5s\n",
      "kube-system   kube-controller-manager-261913e3dab5   0/1     Pending   0          5s\n",
      "kube-system   kube-scheduler-261913e3dab5            0/1     Running   0          6s\n",
      "kube-system   storage-provisioner                    0/1     Pending   0          4s\n"
     ]
    }
   ],
   "source": [
    "# Try to find the GPU device plugin. Not there \n",
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d7385",
   "metadata": {},
   "source": [
    "To install the NVIDIA GPU plugin, we can use the Kubernetes package manager [Helm](https://helm.sh/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c718a440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nvdp\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"nvdp\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n",
      "Release \"nvdp\" does not exist. Installing it now.\n",
      "NAME: nvdp\n",
      "LAST DEPLOYED: Thu Jul 27 07:39:42 2023\n",
      "NAMESPACE: nvidia-device-plugin\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "# Install the device plugin with the Helm package manager\n",
    "!helm repo add nvdp https://nvidia.github.io/k8s-device-plugin \\\n",
    "   && helm repo update\n",
    "!helm upgrade -i nvdp nvdp/nvidia-device-plugin \\\n",
    "  --namespace nvidia-device-plugin \\\n",
    "  --create-namespace \\\n",
    "  --version 0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce43ec9",
   "metadata": {},
   "source": [
    "Check the status again to make sure the plugin was deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90c57655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\n",
      "kube-system   etcd-261913e3dab5                      1/1     Running   0          6s\n",
      "kube-system   kube-apiserver-261913e3dab5            0/1     Running   0          6s\n",
      "kube-system   kube-controller-manager-261913e3dab5   0/1     Pending   0          6s\n",
      "kube-system   kube-scheduler-261913e3dab5            1/1     Running   0          7s\n",
      "kube-system   storage-provisioner                    0/1     Pending   0          5s\n"
     ]
    }
   ],
   "source": [
    "# Now the device plugin \"nvidia-device-plugin-*\" should be \"Running\" after a \"ContainerCreating\" status\n",
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0459b",
   "metadata": {},
   "source": [
    "We should now see the NVIDIA-specific configuration listed against the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc7ae387",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               261913e3dab5\n",
      "Roles:              control-plane,master\n",
      "Labels:             beta.kubernetes.io/arch=amd64\n",
      "                    beta.kubernetes.io/os=linux\n",
      "                    kubernetes.io/arch=amd64\n",
      "                    kubernetes.io/hostname=261913e3dab5\n",
      "                    kubernetes.io/os=linux\n",
      "                    minikube.k8s.io/commit=0a0ad764652082477c00d51d2475284b5d39ceed\n",
      "                    minikube.k8s.io/name=minikube\n",
      "                    minikube.k8s.io/updated_at=2023_07_27T07_39_36_0700\n",
      "                    minikube.k8s.io/version=v1.23.2\n",
      "                    node-role.kubernetes.io/control-plane=\n",
      "                    node-role.kubernetes.io/master=\n",
      "                    node.kubernetes.io/exclude-from-external-load-balancers=\n",
      "Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach: true\n",
      "CreationTimestamp:  Thu, 27 Jul 2023 07:39:33 +0000\n",
      "Taints:             node.kubernetes.io/not-ready:NoSchedule\n",
      "Unschedulable:      false\n",
      "Lease:\n",
      "  HolderIdentity:  261913e3dab5\n",
      "  AcquireTime:     <unset>\n",
      "  RenewTime:       Thu, 27 Jul 2023 07:39:36 +0000\n",
      "Conditions:\n",
      "  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n",
      "  ----             ------  -----------------                 ------------------                ------                       -------\n",
      "  MemoryPressure   False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n",
      "  DiskPressure     False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n",
      "  PIDPressure      False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n",
      "  Ready            False   Thu, 27 Jul 2023 07:39:36 +0000   Thu, 27 Jul 2023 07:39:36 +0000   KubeletNotReady              container runtime status check may not have completed yet\n",
      "Addresses:\n",
      "  InternalIP:  172.18.0.2\n",
      "  Hostname:    261913e3dab5\n",
      "Capacity:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16197972Ki\n",
      "  pods:               110\n",
      "Allocatable:\n",
      "  cpu:                4\n",
      "  ephemeral-storage:  304884524Ki\n",
      "  hugepages-1Gi:      0\n",
      "  hugepages-2Mi:      0\n",
      "  memory:             16197972Ki\n",
      "  pods:               110\n",
      "System Info:\n",
      "  Machine ID:                 996614ec4c814b87b7ec8ebee3d0e8c9\n",
      "  System UUID:                352b2c31-b1a8-4d22-a7dc-e74746eec601\n",
      "  Boot ID:                    e28f248e-77f5-449d-a363-e61b694bca4e\n",
      "  Kernel Version:             5.11.0-1028-aws\n",
      "  OS Image:                   Ubuntu 20.04.5 LTS\n",
      "  Operating System:           linux\n",
      "  Architecture:               amd64\n",
      "  Container Runtime Version:  docker://23.0.2\n",
      "  Kubelet Version:            v1.22.2\n",
      "  Kube-Proxy Version:         v1.22.2\n",
      "Non-terminated Pods:          (4 in total)\n",
      "  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n",
      "  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---\n",
      "  kube-system                 etcd-261913e3dab5                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         6s\n",
      "  kube-system                 kube-apiserver-261913e3dab5             250m (6%)     0 (0%)      0 (0%)           0 (0%)         6s\n",
      "  kube-system                 kube-controller-manager-261913e3dab5    200m (5%)     0 (0%)      0 (0%)           0 (0%)         6s\n",
      "  kube-system                 kube-scheduler-261913e3dab5             100m (2%)     0 (0%)      0 (0%)           0 (0%)         7s\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  Resource           Requests    Limits\n",
      "  --------           --------    ------\n",
      "  cpu                650m (16%)  0 (0%)\n",
      "  memory             100Mi (0%)  0 (0%)\n",
      "  ephemeral-storage  0 (0%)      0 (0%)\n",
      "  hugepages-1Gi      0 (0%)      0 (0%)\n",
      "  hugepages-2Mi      0 (0%)      0 (0%)\n",
      "Events:\n",
      "  Type    Reason                   Age                From     Message\n",
      "  ----    ------                   ----               ----     -------\n",
      "  Normal  NodeHasSufficientMemory  23s (x4 over 23s)  kubelet  Node 261913e3dab5 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    23s (x4 over 23s)  kubelet  Node 261913e3dab5 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     23s (x3 over 23s)  kubelet  Node 261913e3dab5 status is now: NodeHasSufficientPID\n",
      "  Normal  Starting                 6s                 kubelet  Starting kubelet.\n",
      "  Normal  NodeHasSufficientMemory  6s                 kubelet  Node 261913e3dab5 status is now: NodeHasSufficientMemory\n",
      "  Normal  NodeHasNoDiskPressure    6s                 kubelet  Node 261913e3dab5 status is now: NodeHasNoDiskPressure\n",
      "  Normal  NodeHasSufficientPID     6s                 kubelet  Node 261913e3dab5 status is now: NodeHasSufficientPID\n",
      "  Normal  NodeNotReady             6s                 kubelet  Node 261913e3dab5 status is now: NodeNotReady\n",
      "  Normal  NodeAllocatableEnforced  6s                 kubelet  Updated Node Allocatable limit across pods\n"
     ]
    }
   ],
   "source": [
    "# Now we should see Allocable GPUs\n",
    "!kubectl describe nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9149b719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look for the lines containing the word nvidia\n",
    "!kubectl describe nodes | grep nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821e1ea",
   "metadata": {},
   "source": [
    "As we deployed the GPU device plugin, what do you think happened to our application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "255c6e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test\" not found\n"
     ]
    }
   ],
   "source": [
    "# Let's check the application again\n",
    "!kubectl get pods gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f78c3",
   "metadata": {},
   "source": [
    "Our application executed successfully when the GPU resources became available. In fact, it has now completed so we can have a look at its execution logs with `kubectl logs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6af98e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test\" not found\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b40917",
   "metadata": {},
   "source": [
    "Check the list of Helm charts installed with the `helm list` command (see the [Helm documentation](https://helm.sh/docs/helm/helm_list/)). The `--filter` option allows filtering by name.  Use the `--output` option to specify the output format (\"json\", \"table\", or \"yaml\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7b260",
   "metadata": {},
   "source": [
    "Now, let's delete the Kubernetes pod `gpu-operator-test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fa54d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test\" not found\n"
     ]
    }
   ],
   "source": [
    "# Let's delete the pod\n",
    "!kubectl delete pod gpu-operator-test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e356641",
   "metadata": {},
   "source": [
    "Congratulations! You deployed a GPU accelerated applicaiton with Kuberenetes. So far, we have specified that we want a single GPU without specifying which GPU we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccfe3d",
   "metadata": {},
   "source": [
    "--- \n",
    "# 9.4 Interact with GPU Resources in K8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd957491",
   "metadata": {},
   "source": [
    "Now, let's see how to get more control over the GPU-accelerated cluster. Being able to control the GPU type, or the MIG ([Multi-Instance GPU](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)) partition on an Ampere GPU is very important as GPUs vary in terms of computational capability, memory, and cost. The MIG allows users to fragment the GPU into as many as 7 (on A100) partitions. This allows more granular control over the resources in the cluster and better application isolation. \n",
    "\n",
    "In order to control the GPU type, we'll add the `gpu-feature-discovery` plugin and deploy it with Helm. This plugin can be configured with several options, as described in the [gpu-feature-discovery repository](https://github.com/NVIDIA/gpu-feature-discovery#deployment-via-helm). One of the most interesting options when working with Ampere GPUs is the ability to support MIG partitions. The feature discovery plugin can be deployed with the following configurable features:\n",
    "\n",
    "\n",
    "|Feature|Description|Default|\n",
    "|-|-|-|\n",
    "|`failOnInitError`|Fail if there is an error during initialization of any label sources|\"true\"|\n",
    "|`sleepInterval`|Time to sleep between labeling|\"60s\"|\n",
    "|`migStrategy`|Pass the desired strategy for labeling MIG devices on GPUs that support it [none | single | mixed]|\"none\"|\n",
    "|`nfd.deploy`|When set to true, deploy NFD as a subchart with all of the proper parameters set for it|\"true\"|\n",
    "\n",
    "In this class, we are not using Ampere GPUs, so we will do a simple install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a160d703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"nvgfd\" has been added to your repositories\n",
      "Hang tight while we grab the latest from your chart repositories...\n",
      "...Successfully got an update from the \"nvdp\" chart repository\n",
      "...Successfully got an update from the \"nvgfd\" chart repository\n",
      "Update Complete. ⎈Happy Helming!⎈\n",
      "Release \"nvgfd\" does not exist. Installing it now.\n",
      "NAME: nvgfd\n",
      "LAST DEPLOYED: Thu Jul 27 07:39:44 2023\n",
      "NAMESPACE: gpu-feature-discovery\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "# Install feature discovery with the Helm package manager\n",
    "!helm repo add nvgfd https://nvidia.github.io/gpu-feature-discovery \\\n",
    "    && helm repo update\n",
    "!helm upgrade -i nvgfd nvgfd/gpu-feature-discovery \\\n",
    "  --version 0.7.0 \\\n",
    "  --namespace gpu-feature-discovery \\\n",
    "  --create-namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a0bbc",
   "metadata": {},
   "source": [
    "Let's look at additional information that we have about our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f21f8330",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Looking for all of the NVIDIA related information\n",
    "!kubectl describe nodes | grep \"nvidia.com\" -A 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ed633",
   "metadata": {},
   "source": [
    "You should see a wide range of GPU-specific information, including the driver and CUDA information, as well as which GPU is in use from `nvidia.com/gpu.product`.\n",
    "\n",
    "This is probably an NVIDIA A10, unless you are running the class on an alternative GPU. Recall that we deployed our test application `gpu-operator-test` with a generic \"GPU\".  It is possible to deploy it with more specific information regarding the GPU. \n",
    "\n",
    "A new YAML file, `gpu-pod-A10.yaml`, is already prepared. Let's inspect it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e5cb16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: gpu-operator-test-a100\n",
      "spec:\n",
      "  restartPolicy: OnFailure\n",
      "  containers:\n",
      "  - name: cuda-vector-add\n",
      "    image: \"nvidia/samples:vectoradd-cuda11.6.0\"\n",
      "    resources:\n",
      "      limits:\n",
      "         nvidia.com/gpu: 1\n",
      "  nodeSelector: \n",
      "    nvidia.com/gpu.product: A100-SXM4-40GB "
     ]
    }
   ],
   "source": [
    "# Review the application we are deploying\n",
    "!cat $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d931ac",
   "metadata": {},
   "source": [
    "As you might have noticed, the YAML was configured to deploy on an A100 GPU, which is not available in the class. Go ahead and deploy the application anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "001c9c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (Forbidden): error when creating \"/dli/task/kubernetes-config/gpu-pod-A10.yaml\": pods \"gpu-operator-test-a100\" is forbidden: error looking up service account default/default: serviceaccount \"default\" not found\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8747bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test-a100\" not found\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods gpu-operator-test-a100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff274f7",
   "metadata": {},
   "source": [
    "Just as we saw in the earlier non-GPU case, the deployment is in the \"Pending\" state and it will remain in this state until an A100 GPU becomes available or it is terminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f0337",
   "metadata": {},
   "source": [
    "## 9.4.1 Exercise: Configure Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d06e9c0",
   "metadata": {},
   "source": [
    "Modify the YAML file and deploy the `gpu-operator-test` application on the correct GPU.\n",
    "Open the [gpu-pod-A10.yaml](kubernetes-config/gpu-pod-A10.yaml) config file and make those changes:\n",
    "* Change the pod name to \"gpu-operator-test-a10\"\n",
    "* Set the GPU product to the GPU name you found earlier (such as \"NVIDIA-A10G\") instead of the A100\n",
    "\n",
    "Check your work against the [solution](solutions/ex9.4.1.yaml) before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e374560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4c4\n",
      "<   name: gpu-operator-test-a100\n",
      "---\n",
      ">   name: gpu-operator-test-a10\n",
      "14c14\n",
      "<     nvidia.com/gpu.product: A100-SXM4-40GB \n",
      "\\ No newline at end of file\n",
      "---\n",
      ">     nvidia.com/gpu.product: NVIDIA-A10G \n",
      "\\ No newline at end of file\n"
     ]
    }
   ],
   "source": [
    "# TODO modify gpu-pod-A10.yaml so that this cell verifies changes are correct\n",
    "# Check your work - you'll get no output if the files match\n",
    "!diff $CONFIG_DIR/gpu-pod-A10.yaml solutions/ex9.4.1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "573f74e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick Fix!\n",
    "!cp solutions/ex9.4.1.yaml $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfaef3",
   "metadata": {},
   "source": [
    "Next, deploy the `gpu-operator-test-a10` pod using the modified [gpu-pod-A10.yaml](kubernetes-config/gpu-pod-A10.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2e1c5df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (Forbidden): error when creating \"/dli/task/kubernetes-config/gpu-pod-A10.yaml\": pods \"gpu-operator-test-a10\" is forbidden: error looking up service account default/default: serviceaccount \"default\" not found\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f $CONFIG_DIR/gpu-pod-A10.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f71250",
   "metadata": {},
   "source": [
    "## 9.4.2 Final Checks and Shutdown\n",
    "It might take several seconds, but the application should deploy and finish successfully.  Rerun the next cell until the status shows the test is \"completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb4f2f40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test-a10\" not found\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the pod deployed\n",
    "!kubectl get pods gpu-operator-test-a10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f77c639d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"gpu-operator-test-a10\" not found\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the output\n",
    "!kubectl logs gpu-operator-test-a10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb4306",
   "metadata": {},
   "source": [
    "### 9.4.2.1 Exercise: Delete a Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caaab0c",
   "metadata": {},
   "source": [
    "Delete the Kubernetes pod `gpu-operator-test-a10`. Check the [solution](solutions/ex9.4.2.ipynb) before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e2e5694",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubectl controls the Kubernetes cluster manager.\n",
      "\n",
      " Find more information at:\n",
      "https://kubernetes.io/docs/reference/kubectl/overview/\n",
      "\n",
      "Basic Commands (Beginner):\n",
      "  create        Create a resource from a file or from stdin\n",
      "  expose        Take a replication controller, service, deployment or pod and\n",
      "expose it as a new Kubernetes service\n",
      "  run           Run a particular image on the cluster\n",
      "  set           Set specific features on objects\n",
      "\n",
      "Basic Commands (Intermediate):\n",
      "  explain       Get documentation for a resource\n",
      "  get           Display one or many resources\n",
      "  edit          Edit a resource on the server\n",
      "  delete        Delete resources by file names, stdin, resources and names, or\n",
      "by resources and label selector\n",
      "\n",
      "Deploy Commands:\n",
      "  rollout       Manage the rollout of a resource\n",
      "  scale         Set a new size for a deployment, replica set, or replication\n",
      "controller\n",
      "  autoscale     Auto-scale a deployment, replica set, stateful set, or\n",
      "replication controller\n",
      "\n",
      "Cluster Management Commands:\n",
      "  certificate   Modify certificate resources.\n",
      "  cluster-info  Display cluster information\n",
      "  top           Display resource (CPU/memory) usage\n",
      "  cordon        Mark node as unschedulable\n",
      "  uncordon      Mark node as schedulable\n",
      "  drain         Drain node in preparation for maintenance\n",
      "  taint         Update the taints on one or more nodes\n",
      "\n",
      "Troubleshooting and Debugging Commands:\n",
      "  describe      Show details of a specific resource or group of resources\n",
      "  logs          Print the logs for a container in a pod\n",
      "  attach        Attach to a running container\n",
      "  exec          Execute a command in a container\n",
      "  port-forward  Forward one or more local ports to a pod\n",
      "  proxy         Run a proxy to the Kubernetes API server\n",
      "  cp            Copy files and directories to and from containers\n",
      "  auth          Inspect authorization\n",
      "  debug         Create debugging sessions for troubleshooting workloads and\n",
      "nodes\n",
      "\n",
      "Advanced Commands:\n",
      "  diff          Diff the live version against a would-be applied version\n",
      "  apply         Apply a configuration to a resource by file name or stdin\n",
      "  patch         Update fields of a resource\n",
      "  replace       Replace a resource by file name or stdin\n",
      "  wait          Experimental: Wait for a specific condition on one or many\n",
      "resources\n",
      "  kustomize     Build a kustomization target from a directory or URL.\n",
      "\n",
      "Settings Commands:\n",
      "  label         Update the labels on a resource\n",
      "  annotate      Update the annotations on a resource\n",
      "  completion    Output shell completion code for the specified shell (bash or\n",
      "zsh)\n",
      "\n",
      "Other Commands:\n",
      "  api-resources Print the supported API resources on the server\n",
      "  api-versions  Print the supported API versions on the server, in the form of\n",
      "\"group/version\"\n",
      "  config        Modify kubeconfig files\n",
      "  plugin        Provides utilities for interacting with plugins\n",
      "  version       Print the client and server version information\n",
      "\n",
      "Usage:\n",
      "  kubectl [flags] [options]\n",
      "\n",
      "Use \"kubectl <command> --help\" for more information about a given command.\n",
      "Use \"kubectl options\" for a list of global command-line options (applies to all\n",
      "commands).\n"
     ]
    }
   ],
   "source": [
    "# TODO delete the pod\n",
    "!kubectl #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64dd64",
   "metadata": {},
   "source": [
    "Before moving forward to the next notebook, shut down K8s and clean up the docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c23bd1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄  Uninstalling Kubernetes v1.22.2 using kubeadm ...\n",
      "🔥  Deleting \"minikube\" in none ...\n",
      "💀  Removed all traces of the \"minikube\" cluster.\n",
      "\"docker kill\" requires at least 1 argument.\n",
      "See 'docker kill --help'.\n",
      "\n",
      "Usage:  docker kill [OPTIONS] CONTAINER [CONTAINER...]\n",
      "\n",
      "Kill one or more running containers\n",
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "# Shut down K8s\n",
    "!minikube delete\n",
    "# Shut down running docker containers\n",
    "!docker kill $(docker ps -q)\n",
    "# Check for clean environment - this should be empty\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15389a1",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "In this notebook, you have:\n",
    "- Launched a K8s cluster\n",
    "- Interacted with K8s using `kubectl`\n",
    "- Installed plugins with Helm\n",
    "- Enabled GPU acceleration and GPU feature discovery\n",
    "- Deployed an application\n",
    "\n",
    "Next, you'll monitor activity on the cluster. Move on to [Monitoring GPU within Kubernetes Cluster](010_K8s_Monitor.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba9f69",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535681f-abad-45a3-8ed7-e48d2c07f6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
